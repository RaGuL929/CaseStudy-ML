{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,SpatialDropout1D,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "def preprocessing_text(table):\n",
    "    #put everythin in lowercase\n",
    "    table['text'] = table['text'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['text'] = table['text'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['text'] = table['text'].replace(r'@\\w+', '', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['text'] = table['text'].replace(r'http\\S+', '', regex=True)\n",
    "    table['text'] = table['text'].replace(r'www.[^ ]+', '', regex=True)\n",
    "    #remove numbers\n",
    "    table['text'] = table['text'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['text'] = table['text'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    return table\n",
    "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, it will return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row\n",
    "\n",
    "\n",
    "# Removing stop words in twitter feeds NOTE:- the feed should be in LOWER before passing through this function\n",
    "\n",
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    table['text'] = table['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table\n",
    "\n",
    "\n",
    "#replacing negations with antonyms\n",
    "def replace_antonyms(word):\n",
    "    # We get all the lemma for the word\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            # if the lemma is an antonyms of the word\n",
    "            if lemma.antonyms():\n",
    "                # we return the antonym\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "def handling_negation(row):\n",
    "    # Tokenize the row\n",
    "    words = word_tokenize(row)\n",
    "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tags_2 = ''\n",
    "    if \"n't\" in words and \"not\" in words:\n",
    "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words = words[:(min(words.index(\"n't\"), words.index(\"not\"))) + 1]\n",
    "    elif \"n't\" in words:\n",
    "        tags_2 = tags[words.index(\"n't\"):]\n",
    "        words_2 = words[words.index(\"n't\"):]\n",
    "        words = words[:words.index(\"n't\") + 1]\n",
    "    elif \"not\" in words:\n",
    "        tags_2 = tags[words.index(\"not\"):]\n",
    "        words_2 = words[words.index(\"not\"):]\n",
    "        words = words[:words.index(\"not\") + 1]\n",
    "\n",
    "    for index, word_tag in enumerate(tags_2):\n",
    "        if word_tag[1] in speach_tags:\n",
    "            words = words + [replace_antonyms(word_tag[0])] + words_2[index + 2:]\n",
    "            break\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]\n",
    "\n",
    "\n",
    "print(\"*******************Starting data processing*******************\")\n",
    "\n",
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data=preprocessing_text(data)\n",
    "data['text'] = data['text'].apply(lambda x: detect_elongated_words(x))\n",
    "data['text'] = data['text'].apply(lambda x: handling_negation(x))\n",
    "data = stop_words(data)\n",
    "print(\"*******************Completed data processing********************\")\n",
    "\n",
    "\n",
    "\n",
    "max_features = 3500\n",
    "tokenizer = Tokenizer(nb_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "# X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "# X = pad_sequences(X)\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "dictionary=tokenizer.word_index\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in data['text'].values:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "X = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*******************Building Model*******************\")\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "#\n",
    "# model.add(Embedding(max_features, embed_dim,input_length = X_train.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(2, activation='softmax'))\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_features,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*******************Training Model*******************\")\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)\n",
    "model.save('Sen.h5')  #saving weights of neural network and its architecture in the given file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*******************Evalvulating Model*******************\")\n",
    "\n",
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "print(\"*******************Validating Model*******************\")\n",
    "\n",
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1, X_test.shape[1]), batch_size=1, verbose=2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "print(X_validate[x].reshape(1, X_test.shape[1]).shape)\n",
    "print(\"pos_acc\", pos_correct / pos_cnt * 100, \"%\")\n",
    "print(\"neg_acc\", neg_correct / neg_cnt * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
